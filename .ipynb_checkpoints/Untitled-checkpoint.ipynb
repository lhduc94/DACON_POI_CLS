{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9202a376-f1b3-4376-a18f-63f5bafe9d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Vendors\\anaconda\\lib\\site-packages\\pkg_resources\\__init__.py:123: PkgResourcesDeprecationWarning: 4.0.0-unsupported is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "D:\\Vendors\\anaconda\\lib\\site-packages\\pkg_resources\\__init__.py:123: PkgResourcesDeprecationWarning: 4.0.0-unsupported is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "import pytorch_lightning as pl \n",
    "from pytorch_lightning import seed_everything, Trainer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eca7a086-2388-48d4-a9cc-34f69578b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    DIR_NAME='/Data/POI_classification/'\n",
    "    TRAIN_CSV='/Data/POI_classification/train.csv'\n",
    "    TEST_CSV='/Data/POI_classification/test.csv'\n",
    "    TRAIN_IMAGE_DIR='/Data/POI_classification/image/train/'\n",
    "    TEST_IMAGE_DIR='/Data/POI_classification/image/test/'\n",
    "    SAMPLE_SUBMISSION='/Data/POI_classification/sample_submission.csv'\n",
    "    PRETRAINED_PATH='bert-base-multilingual-cased'\n",
    "    MAX_SEQ_LENGTH=152\n",
    "    MAX_TOKEN_LEN=200\n",
    "    SEED=42\n",
    "    NUM_CLASSES=3\n",
    "    HIDDEN_SIZE=512\n",
    "    DROPOUT=0.2\n",
    "    N_TRAINING_DATA=-1\n",
    "    ACCUMULATE_GRAD_BATCHES=1\n",
    "    NUM_TRAIN_EPOCHS=1\n",
    "    WEIGHT_DECAY=0.01\n",
    "    LEARNING_RATE=5e-5\n",
    "    ADAM_EPSILON=1e-8\n",
    "    N_WARMUP_STEP=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83189616-d342-4a0b-8a22-0df3b3fcd286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(cfg.SEED, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7756473-c06b-4481-bb57-b98e62a5ce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(cfg.TRAIN_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12166a68-3bae-4125-b369-cf831b10541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df.cat3.isin(['한식','야영장,오토캠핑장','바/까페'])].sample(30,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "880cae77-b9dd-4f33-8bba-8b5653a5a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'바/까페':0, '한식':1, '야영장,오토캠핑장':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "915b8a4b-c07a-4451-996d-aa5892bcf733",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['cat3_encode'] = train_df['cat3'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cfd204d-fde0-4018-b03f-38aaa0fad109",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.PRETRAINED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88436080-70ac-4741-9d5e-0cbcc3d49d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(train_df, train_size=0.8, random_state=cfg.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b70b78df-c292-4a80-9bb4-a479330bb11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverviewData(Dataset):\n",
    "    def __init__(self, data, tokenizer:AutoTokenizer):\n",
    "        self.examples = None\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.processing()\n",
    "    def processing(self):\n",
    "        processed_data  = {}\n",
    "        texts = [[x] for x in self.data.overview.values]\n",
    "        self.examples = self.tokenizer(texts,\n",
    "                                       truncation=True,\n",
    "                                       padding='max_length',\n",
    "                                       max_length=cfg.MAX_TOKEN_LEN,\n",
    "                                       is_split_into_words=True,\n",
    "                                       return_tensors='pt')\n",
    "        self.examples['labels'] = self.data.cat3_encode.values\n",
    "        self.examples['texts'] = self.data.overview.values\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'input_ids': self.examples['input_ids'][index],\n",
    "            'attention_mask': self.examples['attention_mask'][index],\n",
    "            'label': self.examples['labels'][index],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "808d110a-a5e0-440c-96f7-83addba30df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = OverviewData(train, tokenizer)\n",
    "valid_dataset = OverviewData(valid, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eabad6f-8357-49aa-bbec-f9829b4563e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=1)\n",
    "val_dataloader = DataLoader(valid_dataset, batch_size=8, shuffle=False,  num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d63cb1ea-e03a-4441-858e-7d00bd0b98d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.N_TRAINING_DATA = len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3cc697f-90b3-4734-aa28-3acdde32d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLSModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.num_classes = cfg.NUM_CLASSES\n",
    "        self.model = AutoModel.from_pretrained(cfg.PRETRAINED_PATH)\n",
    "        self.hidden_size = self.model.config.hidden_size\n",
    "        self.dropout_prob = self.model.config.hidden_dropout_prob\n",
    "        self.classifier = nn.Sequential(\n",
    "                            nn.Linear(self.hidden_size, cfg.NUM_CLASSES))\n",
    "        # self.init_weights()\n",
    "        print('--Init--')\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        print('forward')\n",
    "        output = self.model(input_ids, attention_mask)\n",
    "        logits = self.classifier(output.pooler_output)\n",
    "    \n",
    "        loss = 0\n",
    "        if labels:\n",
    "            loss = self.calc_loss(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "    def calc_loss(self, logits, labels):\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return loss\n",
    "            \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        input_ids =  train_batch[\"input_ids\"]\n",
    "        # print(input_ids)\n",
    "        attention_mask = train_batch[\"attention_mask\"]\n",
    "        labels = train_batch[\"label\"]\n",
    "        print('hihe')\n",
    "        loss, logits = self(input_ids, attention_mask, labels)\n",
    "        predictions = logits.argmax(1)\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        return {\"loss\": loss, \"predictions\": predictions, \"labels\": labels}\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        input_ids =  val_batch[\"input_ids\"]\n",
    "        attention_mask = val_batch[\"attention_mask\"]\n",
    "        labels = val_batch[\"label\"]\n",
    "        loss, logits = self(input_ids, attention_mask, labels)\n",
    "        predictions = logits.argmax(1)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "        return  {\"loss\": loss, \"predictions\": predictions, \"labels\": labels}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        for output in outputs:\n",
    "            for out_labels in output['labels'].detach().cpu():\n",
    "                labels.append(out_labels)\n",
    "            for out_predictions in output['prediction'].detach().cpu():\n",
    "                predictions.append(out_prediction)\n",
    "        labels = torch.stack(labels)\n",
    "        predictions = torch.stack(predictions)\n",
    "        score = f1_score(labels, predictions, average='weighted')\n",
    "        \n",
    "        self.log('train f1-score', score)\n",
    "        return loss,score\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        for output in outputs:\n",
    "            for out_labels in output['labels'].detach().cpu():\n",
    "                labels.append(out_labels)\n",
    "            for out_predictions in output['prediction'].detach().cpu():\n",
    "                predictions.append(out_prediction)\n",
    "        labels = torch.stack(labels)\n",
    "        predictions = torch.stack(predictions)\n",
    "        score = f1_score(labels, predictions, average='weighted')\n",
    "        \n",
    "        self.log('valid f1-score', score)\n",
    "        \n",
    "        return loss, score\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        total_step = self.cfg.N_TRAINING_DATA // self.cfg.ACCUMULATE_GRAD_BATCHES * self.cfg.NUM_TRAIN_EPOCHS\n",
    "        \n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.cfg.WEIGHT_DECAY,\n",
    "            },\n",
    "            {\"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, \n",
    "                          lr=self.cfg.LEARNING_RATE, \n",
    "                          eps=self.cfg.ADAM_EPSILON)\n",
    "        scheduler =  get_linear_schedule_with_warmup(optimizer, \n",
    "                                                     num_warmup_steps=self.cfg.N_WARMUP_STEP,\n",
    "                                                     num_training_steps=total_step)\n",
    "        \n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fb14fea-9a35-48ec-8d8d-024496f8bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(cfg.PRETRAINED_PATH)\n",
    "output = model(train_dataset[:]['input_ids'], train_dataset[:]['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba439f7d-1036-4989-8ac3-2c1acd07710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "tb_logger = TensorBoardLogger('logs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b4c24d3-5d0b-4881-8d9c-dc2b746c606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath='models',\n",
    "        save_top_k=1,\n",
    "        monitor='valid f1-score',\n",
    "        mode='max',\n",
    "        save_weights_only=True,\n",
    "        filename='{epoch}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d5a635c-c663-49dd-90a0-7de742b2a387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Init--\n"
     ]
    }
   ],
   "source": [
    "model = CLSModel(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d7fae7-a7b9-4245-bfc9-9472dc8be62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Vendors\\anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:712: UserWarning: You passed `Trainer(accelerator='cpu', precision=16)` but native AMP is not supported on CPU. Using `precision='bf16'` instead.\n",
      "  rank_zero_warn(\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: logs/lightning_logs\n",
      "D:\\Vendors\\anaconda\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | model      | BertModel  | 177 M \n",
      "1 | classifier | Sequential | 2.3 K \n",
      "------------------------------------------\n",
      "177 M     Trainable params\n",
      "0         Non-trainable params\n",
      "177 M     Total params\n",
      "711.423   Total estimated model params size (MB)\n",
      "D:\\Vendors\\anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "D:\\Vendors\\anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:219: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc4df2f6ff04064af9ecc8616e2351e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        logger=tb_logger,\n",
    "        accelerator='cpu',\n",
    "        callbacks=[checkpoint_callback],\n",
    "        gradient_clip_val=1,\n",
    "        accumulate_grad_batches=cfg.ACCUMULATE_GRAD_BATCHES,\n",
    "        max_epochs=cfg.NUM_TRAIN_EPOCHS,\n",
    "        precision=16,\n",
    "        log_every_n_steps=1,\n",
    "        num_sanity_val_steps=0\n",
    "    )\n",
    "    \n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=[val_dataloader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e87520a-db2e-4454-8421-517efde42a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
